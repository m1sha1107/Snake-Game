			## initializations

tf.constant(val, *name, *dtype)
tf.Variable(val, *name, *dtype)
tf.zeros()
tf.ones()


			## dataset
#attributes
x.element_spec
x.map(func) # func should take single tens


			## operations

tf.add(x , y)\

tf.matmul(x, y)

			## meth functions

tf.keras.activations.sigmoid() # takes only floats and complex
tf.keras.activations.softmax()
tf.keras.activations.relu()

tf.keras.categorical_crossentropy(*y_pred, *y_true)

			##GRADGRADGRAD
tape = tf.gradient tape()
tap.gradient(cost, trainable_variables)

			## optimizers
tf.keras.optimizers.Adam(learning_rate) #optimizer instance

#use
optimizer.apply_gradients(zip(grads, trainable_variables))



			## useful crap

tf.cast(tensor, tf.float32)
tf.reshape(tensor, dimlist)
tf.transpose(x)


tf.reduce_mean(tens, *axis)
tf.one_hot(x, depth, *axis)

x.prefetch(2^n)
			## crap you will never use

tf.keras.initializers.GlorotNormal(*seed) #create initializer obj. use in val of tf.Variable
